{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7486c1-c401-42ae-bebf-cfff8d5cb897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Fist of we will upload our data, which is taken from kaggle and load it to Hadoop\n",
    "\n",
    "#for uploading the data we will use data ingestion\n",
    "\n",
    "#Step 1: First of of we will ge the data from kaggle by using curl command\n",
    "#1: Make a new directory as olist by using mkdir olist\n",
    "#2: Now by using curl command for dumping the data to using olist directory\n",
    "# curl -L -o ~/olist/brazilian-ecommerce.zip\\\n",
    "#  https://www.kaggle.com/api/v1/datasets/download/olistbr/brazilian-ecommerce\n",
    "\n",
    "#step 2: Now we will put this data to hadoop master cluster \n",
    "# we can use folllowing command hdfs fs -put /olist/\n",
    "\n",
    "#Step3: Create another directory as data fro unzipping the zip file\n",
    "#unzip brazalian-ecommerce.zip -d ~/olist/data/\n",
    "\n",
    "# Step 4: Now put data to hadoop cluster\n",
    "# first create a diresctory by hdfs dfs -mkdir/data\n",
    "# hdfs dfs -mkdir/data/olist/\n",
    "\n",
    "# Data Ingestion\n",
    "# hadoop fs -put ~/olist/data/*.csv /data/olist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df8ed67-c9fe-4f2d-987e-3bffd8bcb7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/12 06:40:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    ".appName(\"E-commerce data analysis\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b27de2d-9730-45f1-b519-87b97fbd2dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e-commerce-analysis-m.us-central1-a.c.logical-iridium-462512-d7.internal:35875\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3767b6ff50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e97795-eb0b-47df-a275-0e07e428ac9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\n",
      "-rw-r--r--   2 niteshsoni30 hadoop    9033957 2025-06-11 15:34 /data/olist/olist_customers_dataset.csv\n",
      "-rw-r--r--   2 niteshsoni30 hadoop   61273883 2025-06-11 15:34 /data/olist/olist_geolocation_dataset.csv\n",
      "-rw-r--r--   2 niteshsoni30 hadoop   15438671 2025-06-11 15:34 /data/olist/olist_order_items_dataset.csv\n",
      "-rw-r--r--   2 niteshsoni30 hadoop    5777138 2025-06-11 15:34 /data/olist/olist_order_payments_dataset.csv\n",
      "-rw-r--r--   2 niteshsoni30 hadoop   14451670 2025-06-11 15:34 /data/olist/olist_order_reviews_dataset.csv\n",
      "-rw-r--r--   2 niteshsoni30 hadoop   17654914 2025-06-11 15:34 /data/olist/olist_orders_dataset.csv\n",
      "-rw-r--r--   2 niteshsoni30 hadoop    2379446 2025-06-11 15:34 /data/olist/olist_products_dataset.csv\n",
      "-rw-r--r--   2 niteshsoni30 hadoop     174703 2025-06-11 15:34 /data/olist/olist_sellers_dataset.csv\n",
      "-rw-r--r--   2 niteshsoni30 hadoop       2613 2025-06-11 15:34 /data/olist/product_category_name_translation.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /data/olist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e61fa0-f207-4f88-b570-9dc06c7c7733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdfs_path = \"/data/olist/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d798d74d-028a-48c5-a7ee-4da4fa5614c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customers_df = spark.read.csv(hdfs_path + \"olist_customers_dataset.csv\", header = True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a66e48-58a3-46d9-9ca8-3d784f52a343",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|         customer_id|  customer_unique_id|customer_zip_code_prefix|       customer_city|customer_state|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|06b8999e2fba1a1fb...|861eff4711a542e4b...|                   14409|              franca|            SP|\n",
      "|18955e83d337fd6b2...|290c77bc529b7ac93...|                    9790|sao bernardo do c...|            SP|\n",
      "|4e7b3e00288586ebd...|060e732b5b29e8181...|                    1151|           sao paulo|            SP|\n",
      "|b2b6027bc5c5109e5...|259dac757896d24d7...|                    8775|     mogi das cruzes|            SP|\n",
      "|4f2d8ab171c80ec83...|345ecd01c38d18a90...|                   13056|            campinas|            SP|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e64fe303-8dc9-48c7-8a5c-50c7b1ea6ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "product_category_df = spark.read.csv(hdfs_path + \"product_category_name_translation.csv\", header = True, inferSchema= True)\n",
    "geolocation_df = spark.read.csv(hdfs_path + \"olist_geolocation_dataset.csv\", header = True, inferSchema= True)\n",
    "order_items_df = spark.read.csv(hdfs_path + \"olist_order_items_dataset.csv\", header = True, inferSchema= True)\n",
    "payments_df = spark.read.csv(hdfs_path + \"olist_order_payments_dataset.csv\", header = True, inferSchema= True)\n",
    "reviews_df = spark.read.csv(hdfs_path + \"olist_order_reviews_dataset.csv\", header = True, inferSchema= True)\n",
    "orders_df = spark.read.csv(hdfs_path + \"olist_orders_dataset.csv\", header = True, inferSchema= True)\n",
    "sellers_df = spark.read.csv(hdfs_path + \"olist_sellers_dataset.csv\", header = True, inferSchema= True)\n",
    "products_df = spark.read.csv(hdfs_path + \"olist_products_dataset.csv\", header = True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6684ca-5cbe-4979-96f6-2acf565a9640",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: integer (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17458932-f4ee-4bb1-8198-d3837ebce15c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuatomers : 99441 rows\n"
     ]
    }
   ],
   "source": [
    "#Data Leakage or drop\n",
    "\n",
    "print(f'cuatomers : {customers_df.count()} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "befa3341-4d4f-4fc8-af52-f67383982f37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------------------+-------------+--------------+\n",
      "|customer_id|customer_unique_id|customer_zip_code_prefix|customer_city|customer_state|\n",
      "+-----------+------------------+------------------------+-------------+--------------+\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "|      false|             false|                   false|        false|         false|\n",
      "+-----------+------------------+------------------------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#null or duplicate values\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "customers_df.select([col(c).isNull().alias(c) for c in customers_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39b17c2a-d081-45f5-81bd-0ea2833ce02a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------------------+-------------+--------------+\n",
      "|customer_id|customer_unique_id|customer_zip_code_prefix|customer_city|customer_state|\n",
      "+-----------+------------------+------------------------+-------------+--------------+\n",
      "|          0|                 0|                       0|            0|             0|\n",
      "+-----------+------------------+------------------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "customers_df.select([count(when(col(c).isNull(), 1)).alias(c) for c in customers_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "360c734b-6711-4866-b9ca-60e90efd7022",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Duplicate Values\n",
    "\n",
    "customers_df.groupby(\"customer_id\").count().filter(\"count>1\").show()\n",
    "\n",
    "# As we can observe that there is no duplicatee value present in out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8db08784-cbbc-4c65-90b3-7de67aa8aa1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|customer_state|count|\n",
      "+--------------+-----+\n",
      "|            SP|41746|\n",
      "|            RJ|12852|\n",
      "|            MG|11635|\n",
      "|            RS| 5466|\n",
      "|            PR| 5045|\n",
      "|            SC| 3637|\n",
      "|            BA| 3380|\n",
      "|            DF| 2140|\n",
      "|            ES| 2033|\n",
      "|            GO| 2020|\n",
      "|            PE| 1652|\n",
      "|            CE| 1336|\n",
      "|            PA|  975|\n",
      "|            MT|  907|\n",
      "|            MA|  747|\n",
      "|            MS|  715|\n",
      "|            PB|  536|\n",
      "|            PI|  495|\n",
      "|            RN|  485|\n",
      "|            AL|  413|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Customer Distribution by sales data\n",
    "\n",
    "customers_df.groupby(\"customer_state\").count().orderBy(\"count\",ascending= False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c8a2e3e-650b-4cdf-a3be-952136ba8172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|order_status|count|\n",
      "+------------+-----+\n",
      "|   delivered|96478|\n",
      "|     shipped| 1107|\n",
      "|    canceled|  625|\n",
      "| unavailable|  609|\n",
      "|    invoiced|  314|\n",
      "|  processing|  301|\n",
      "|     created|    5|\n",
      "|    approved|    2|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ORDER DATA: Order Status Distribution\n",
    "\n",
    "orders_df.groupBy(\"order_status\").count().orderBy(\"count\", ascending= False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "874833aa-30f4-49e6-848a-42507652bacb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------+--------------------+-------------+\n",
      "|            order_id|payment_sequential|payment_type|payment_installments|payment_value|\n",
      "+--------------------+------------------+------------+--------------------+-------------+\n",
      "|b81ef226f3fe1789b...|                 1| credit_card|                   8|        99.33|\n",
      "|a9810da82917af2d9...|                 1| credit_card|                   1|        24.39|\n",
      "|25e8ea4e93396b6fa...|                 1| credit_card|                   1|        65.71|\n",
      "|ba78997921bbcdc13...|                 1| credit_card|                   8|       107.78|\n",
      "|42fdf880ba16b47b5...|                 1| credit_card|                   2|       128.45|\n",
      "+--------------------+------------------+------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Payments\n",
    "    \n",
    "payments_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9915fb11-5360-4924-a5a3-498f57f7e780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|payment_type|count|\n",
      "+------------+-----+\n",
      "| credit_card|76795|\n",
      "|      boleto|19784|\n",
      "|     voucher| 5775|\n",
      "|  debit_card| 1529|\n",
      "| not_defined|    3|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Payment Distribution\n",
    "\n",
    "payments_df.groupBy(\"payment_type\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4ca1b3d-366c-4896-98b5-1dd737d8d1d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|\n",
      "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|\n",
      "|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|\n",
      "|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|\n",
      "|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80e5f1af-6c51-47aa-a35b-8a7cca66a259",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|          product_id|       total sales|\n",
      "+--------------------+------------------+\n",
      "|bb50f2e236e5eea01...|           63885.0|\n",
      "|6cdd53843498f9289...| 54730.20000000005|\n",
      "|d6160fb7873f18409...|48899.340000000004|\n",
      "|d1c427060a0f73f6b...| 47214.51000000006|\n",
      "|99a4788cb24856965...|43025.560000000085|\n",
      "|3dd2a17168ec895c7...| 41082.60000000005|\n",
      "|25c38557cf793876c...| 38907.32000000001|\n",
      "|5f504b3a1c75b73d6...|37733.899999999994|\n",
      "|53b36df67ebb7c415...| 37683.42000000001|\n",
      "|aca2eb7d00ea1a7b8...| 37608.90000000007|\n",
      "|e0d64dcfaa3b6db5c...|          31786.82|\n",
      "|d285360f29ac7fd97...|31623.809999999983|\n",
      "|7a10781637204d8d1...|           30467.5|\n",
      "|f1c7f353075ce59d8...|          29997.36|\n",
      "|f819f0c84a64f02d3...|29024.479999999996|\n",
      "|588531f8ec37e7d5f...|28291.989999999998|\n",
      "|422879e10f4668299...|26577.219999999972|\n",
      "|16c4e87b98a9370a9...|           25034.0|\n",
      "|5a848e4ab52fd5445...|24229.029999999962|\n",
      "|a62e25e09e05e6faf...|           24051.0|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Top Selling Product\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "top_products = order_items_df.groupBy(\"product_id\").agg(sum(\"price\").alias(\"total sales\"))\n",
    "top_products.orderBy(\"total sales\",ascending= False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aec5a92-6403-40fa-92af-d75e961e0edb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n",
      "|            order_id|         customer_id|order_status|order_purchase_timestamp|  order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|\n",
      "+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n",
      "|e481f51cbdc54678b...|9ef432eb625129730...|   delivered|     2017-10-02 10:56:33|2017-10-02 11:07:15|         2017-10-04 19:55:00|          2017-10-10 21:25:13|          2017-10-18 00:00:00|\n",
      "|53cdb2fc8bc7dce0b...|b0830fb4747a6c6d2...|   delivered|     2018-07-24 20:41:37|2018-07-26 03:24:27|         2018-07-26 14:31:00|          2018-08-07 15:27:45|          2018-08-13 00:00:00|\n",
      "|47770eb9100c2d0c4...|41ce2a54c0b03bf34...|   delivered|     2018-08-08 08:38:49|2018-08-08 08:55:23|         2018-08-08 13:50:00|          2018-08-17 18:06:29|          2018-09-04 00:00:00|\n",
      "|949d5b44dbf5de918...|f88197465ea7920ad...|   delivered|     2017-11-18 19:28:06|2017-11-18 19:45:59|         2017-11-22 13:39:59|          2017-12-02 00:28:42|          2017-12-15 00:00:00|\n",
      "|ad21c59c0840e6cb8...|8ab97904e6daea886...|   delivered|     2018-02-13 21:18:39|2018-02-13 22:20:29|         2018-02-14 19:46:34|          2018-02-16 18:17:02|          2018-02-26 00:00:00|\n",
      "+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average Delivert Time Analysis\n",
    "\n",
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4e7a965-7514-4c31-8033-d6756b729108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delivery_df = orders_df.select('order_id','order_purchase_timestamp','order_delivered_customer_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e88d3b18-315c-4fbf-b7ee-f3fd3f4e9416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff,to_date,col,expr\n",
    "\n",
    "delivery_detail_df = delivery_df.withColumn(\"delivery_time\", datediff(col(\"order_delivered_customer_date\"),col(\"order_purchase_timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fb209c2-6b8d-4e27-b8ba-7f9558177eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+-----------------------------+-------------+\n",
      "|            order_id|order_purchase_timestamp|order_delivered_customer_date|delivery_time|\n",
      "+--------------------+------------------------+-----------------------------+-------------+\n",
      "|ca07593549f1816d2...|     2017-02-21 23:31:27|          2017-09-19 14:36:39|          210|\n",
      "|1b3190b2dfa9d789e...|     2018-02-23 14:57:35|          2018-09-19 23:24:07|          208|\n",
      "|440d0d17af552815d...|     2017-03-07 23:59:51|          2017-09-19 15:12:50|          196|\n",
      "|2fb597c2f772eca01...|     2017-03-08 18:09:02|          2017-09-19 14:33:17|          195|\n",
      "|285ab9426d6982034...|     2017-03-08 22:47:40|          2017-09-19 14:00:04|          195|\n",
      "|0f4519c5f1c541dde...|     2017-03-09 13:26:57|          2017-09-19 14:38:21|          194|\n",
      "|47b40429ed8cce3ae...|     2018-01-03 09:44:01|          2018-07-13 20:51:31|          191|\n",
      "|2fe324febf907e3ea...|     2017-03-13 20:17:10|          2017-09-19 17:00:07|          190|\n",
      "|2d7561026d542c8db...|     2017-03-15 11:24:27|          2017-09-19 14:38:18|          188|\n",
      "|c27815f7e3dd0b926...|     2017-03-15 23:23:17|          2017-09-19 17:14:25|          188|\n",
      "+--------------------+------------------------+-----------------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delivery_detail_df.orderBy(\"delivery_time\",ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f00cecac-1293-47e4-97ad-a8f55358e6b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b05a16c-87e5-4d51-9909-b9013371ff5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
